{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "alive-hungary",
   "metadata": {},
   "source": [
    "Preconditioning\n",
    "===\n",
    "\n",
    "We call $C$ a preconditioner to the matrix $A$ if \n",
    "* $C^{-1} A \\approx I$\n",
    "* the matrix-vector multiplication $w = C^{-1} r$ is cheap\n",
    "\n",
    "One extreme case is $C = A$, where the first claim is optimally satisfied, but (in general) not the second. The opposite extreme is $C = I$. \n",
    "\n",
    "If $A$ is an SPD matrix, we like to have an SPD preconditioner $C$. In this case, the quality of the approximation can be measured by the spectral bounds\n",
    "\n",
    "$$\n",
    "0 < \\gamma_1 \\leq \\frac{x^T A x }{x^T C x} \\leq \\gamma_2 \\qquad \\forall \\, 0 \\neq x \\in {\\mathbb R^n}\n",
    "$$\n",
    "\n",
    "The Rayleigh quotient is bounded by $\\gamma_1$ and $\\gamma_2$ from below and from above.\n",
    "These spectral bounds are bounds for the eigenvalues $\\lambda$ of the generalized eigenvalue problem\n",
    "\n",
    "$$\n",
    "A x = \\lambda C x\n",
    "$$\n",
    "\n",
    "If $\\lambda_i$ is an eigenvalue with eigenvector $x_i$, then $x_i^T A x_i = \\lambda_i x_i^T C x_i$, and thus $\\lambda_i \\in [\\gamma_1, \\gamma_2]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-popularity",
   "metadata": {},
   "source": [
    "The preconditioned Richardson iteration\n",
    "---\n",
    "We use the preconditioner to obtain the correction from the residuum:\n",
    "\n",
    "$$\n",
    "\\qquad  x^{k+1} = x^k - \\alpha C^{-1} (b - A x^k)\n",
    "$$\n",
    "\n",
    "The error is now propagated as\n",
    "\n",
    "$$\n",
    "e^{k+1} = M e^k = (I - \\alpha C^{-1} A) e^k\n",
    "$$\n",
    "\n",
    "If we could use the ideal preconditioner $C = A$, and set $\\alpha = 1$, then $M = 0$, and we converge in one iteration.\n",
    "\n",
    "\n",
    "The error-propagation matrix $M$ is self-adjoint in the energy inner product\n",
    "\\begin{eqnarray*}\n",
    "\\left< M x, y \\right>_A & = & (A M x)^T y \\\\\n",
    "& = & \\left\\{ A (x - \\alpha C^{-1} A x) \\right\\}^T y  \\\\\n",
    "& = & x^T (A - \\alpha A C^{-1} A) y \\\\\n",
    "& = & \\left< x, M y \\right>_A\n",
    "\\end{eqnarray*}\n",
    "\n",
    "as well as in the inner products\n",
    "\n",
    "$$\n",
    "\\left< x, y \\right>_C \\qquad \\text{and} \\qquad \\left< x, y \\right>_{AC^{-1} A}.\n",
    "$$\n",
    "\n",
    "The error is monotonically decreased in the corresponding norms. In particular the last one is practically interesting since it is computationally available:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\| x - x^\\ast \\|_{AC^{-1} A}^2 & = & \\| A (x - x^\\ast) \\|_{C^{-1}}^2 \\\\\n",
    "& = & \\| A x - b \\|_{C^{-1}}^2\n",
    "\\end{eqnarray*}\n",
    "\n",
    "With the residuum $r$ and preconditioned residuum $w$, i.e.\n",
    "\n",
    "$$\n",
    "r = b - A x \\qquad \\text{and} \\qquad w = C^{-1} r\n",
    "$$\n",
    "\n",
    "the error becomes\n",
    "\n",
    "$$\n",
    "\\| x - x^\\ast \\|_{AC^{-1}A}^2 = r^T w\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngsolve import *\n",
    "from netgen.geom2d import unit_square\n",
    "mesh = Mesh(unit_square.GenerateMesh(maxh=0.1))\n",
    "fes = H1(mesh, order=1)\n",
    "u,v = fes.TnT()\n",
    "a = BilinearForm(grad(u)*grad(v)*dx+10*u*v*dx).Assemble()\n",
    "f = LinearForm(x*y*v*dx).Assemble()\n",
    "gfu = GridFunction(fes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-audit",
   "metadata": {},
   "source": [
    "A very simple preconditioner is the Jacobi-preconditioner\n",
    "\n",
    "$$\n",
    "C = \\text{diag} A\n",
    "$$ \n",
    "\n",
    "If $A$ is SPD, then all diagonal entries are positive, and $C$ is SPD as well.\n",
    "\n",
    "In NGSolve, we can obtain a Jacobi preconditioner as follows. The result is a linear operator providing the linear operation\n",
    "\n",
    "$$\n",
    "w := C^{-1} * r\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-pleasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = a.mat.CreateSmoother()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv = gfu.vec.CreateVector()\n",
    "hv2 = gfu.vec.CreateVector()\n",
    "hv3 = gfu.vec.CreateVector()\n",
    "hv.SetRandom()\n",
    "hv.data /= Norm(hv)\n",
    "for k in range(20):\n",
    "    hv2.data = a.mat * hv\n",
    "    hv3.data = pre * hv2\n",
    "    rho = Norm(hv3)\n",
    "    print (rho)\n",
    "    hv.data = 1/rho * hv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1 / rho\n",
    "r = f.vec.CreateVector()\n",
    "w = f.vec.CreateVector()\n",
    "gfu.vec[:] = 0\n",
    "\n",
    "w.data = pre * f.vec\n",
    "err0 = sqrt(InnerProduct(f.vec, w))\n",
    "its = 0\n",
    "while True:\n",
    "    r.data = f.vec - a.mat * gfu.vec\n",
    "    w.data = pre * r\n",
    "    err = sqrt(InnerProduct(r,w))\n",
    "    print (\"iteration\", its, \"res=\", err)\n",
    "    gfu.vec.data += alpha * w\n",
    "    if err < 1e-8 * err0 or its > 10000: break\n",
    "    its = its+1\n",
    "print (\"needed\", its, \"iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-stretch",
   "metadata": {},
   "source": [
    "By situation is not considerably improved by the diagonal preconditioner. However, if we have a bilinear-form with variable coefficient, or large coefficients in the Robin - boundary condition such as\n",
    "\n",
    "$$\n",
    "A(u,v) = \\int_\\Omega \\nabla u \\nabla v \\, dx + 10^8 \\int_{\\Gamma_R} u v \\, ds,\n",
    "$$\n",
    "\n",
    "the Jacobi preconditioner captures these parameters (experiment in  excercise, theory soon)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-sheriff",
   "metadata": {},
   "source": [
    "The preconditioned gradient method\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-enzyme",
   "metadata": {},
   "source": [
    "To introduce the preconditioner into the gradient method we proceed as follows. Since $C$ is SPD, we are allowed to form its square-root\n",
    "\n",
    "$$\n",
    "C^{1/2}\n",
    "$$\n",
    "\n",
    "as well as its inverse. The linear system $A x = b$ is equivalent to \n",
    "\n",
    "$$\n",
    "C^{-1/2} A C^{-1/2} \\; C^{1/2} x = C^{-1/2} b.\n",
    "$$\n",
    "\n",
    "With the definition of transformed quantities \n",
    "\n",
    "$$\n",
    "\\tilde A = C^{-1/2} A C^{-1/2}, \\qquad\n",
    "\\tilde b = C^{-1/2} b, \\qquad\n",
    "\\tilde x = C^{1/2} x\n",
    "$$\n",
    "\n",
    "we have the linear system\n",
    "\n",
    "$$\n",
    "\\tilde A \\tilde x = \\tilde b\n",
    "$$\n",
    "\n",
    "The transformed matrix $\\tilde A$ is SPD as well.\n",
    "\n",
    "We apply the gradient method for the transformed system:\n",
    "\n",
    "Given $\\tilde x^0$ <br>\n",
    "$\\tilde r^0 = \\tilde b - \\tilde A \\tilde x^0$ <br>\n",
    "for $k = 0, 1, 2, \\ldots$ <br>\n",
    "$\\qquad \\tilde p = \\tilde A \\tilde r^k$ <br>\n",
    "$\\qquad \\alpha = \\frac{{\\tilde r^k}^T \\tilde r^k }{ {\\tilde r^k}^T \\tilde p}$ <br>\n",
    "$\\qquad \\tilde x^{k+1} = \\tilde x^k + \\alpha \\tilde r^k$ <br>\n",
    "$\\qquad \\tilde r^{k+1} = \\tilde r^k - \\alpha \\tilde p$ <br>\n",
    "\n",
    "\n",
    "The error analysis of the gradient method provides\n",
    "\n",
    "$$\n",
    "\\| \\tilde x^\\ast - \\tilde x^{k+1} \\|_{\\tilde A} \\leq \\rho\n",
    "\\| \\tilde x^\\ast - \\tilde x^{k} \\|_{\\tilde A},\n",
    "$$\n",
    "\n",
    "and substituting back\n",
    "\n",
    "$$\n",
    "\\| x^\\ast -  x^{k+1} \\|_A \\leq \\rho \\|  x^\\ast -  x^{k} \\|_A\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-combine",
   "metadata": {},
   "source": [
    "Since we cannot compute with the transformed system \n",
    "we transform back via\n",
    "\n",
    "$$\n",
    "\\tilde x^k = C^{1/2} x^k, \\qquad \\tilde r^k = C^{-1/2} r, \n",
    "\\qquad \\tilde p = C^{-1/2} p\n",
    "$$\n",
    "\n",
    "and obtain\n",
    "\n",
    "Given $x^0$ <br>\n",
    "$C^{-1/2} r^0 = C^{-1/2} b  - C^{-1/2} A C^{-1/2} C^{1/2} x^0$ <br>\n",
    "for $k = 0, 1, 2, \\ldots$ <br>\n",
    "$\\qquad C^{-1/2} p = C^{-1/2} A C^{-1/2} C^{-1/2} r^k$ <br>\n",
    "$\\qquad \\alpha = \\frac{\\{C^{-1/2} r^k \\}^T C^{-1/2} r^k \\, }{ \\, \\{ C^{-1/2} r^k\\}^T C^{-1/2} p}$ <br>\n",
    "$\\qquad C^{1/2} x^{k+1} = C^{1/2} x^k + \\alpha C^{-1/2} r^k$ <br>\n",
    "$\\qquad C^{-1/2} r^{k+1} = C^{-1/2} r^k - \\alpha C^{-1/2} p$ <br>\n",
    "\n",
    "now we simplify, and introduce $w = C^{-1} r$\n",
    "\n",
    "Given $x^0$ <br>\n",
    "$r^0 = b - A x^0$ <br>\n",
    "for $k = 0, 1, 2, \\ldots$ <br>\n",
    "$\\qquad w = C^{-1} r^k$ <br>\n",
    "$\\qquad p = A w$ <br>\n",
    "$\\qquad \\alpha = \\frac{w^T r^k}{w^T p^k}$ <br>\n",
    "$\\qquad x^{k+1} = x^k + \\alpha w$ <br>\n",
    "$\\qquad r^{k+1} = r^k - \\alpha p$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = f.vec.CreateVector()\n",
    "w = f.vec.CreateVector()\n",
    "p = f.vec.CreateVector()\n",
    "\n",
    "gfu.vec[:] = 0\n",
    "r.data = f.vec\n",
    "w.data = pre*r\n",
    "err0 = sqrt(InnerProduct(r,w))\n",
    "its = 0\n",
    "while True:\n",
    "    w.data = pre*r\n",
    "    p.data = a.mat * w\n",
    "    err2 = InnerProduct(w,r)\n",
    "    alpha = err2 / InnerProduct(w,p)\n",
    "\n",
    "    print (\"iteration\", its, \"res=\", sqrt(err2))\n",
    "    gfu.vec.data += alpha * w\n",
    "    r.data -= alpha * p\n",
    "    if sqrt(err2) < 1e-8 * err0 or its > 10000: break\n",
    "    its = its+1\n",
    "print (\"needed\", its, \"iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-regulation",
   "metadata": {},
   "source": [
    "Jacobi and Gauss Seidel Preconditioners\n",
    "===\n",
    "For a given residual $r$, the Jacobi preconditioner computes\n",
    "\n",
    "$$\n",
    "w = D^{-1} r\n",
    "$$\n",
    "\n",
    "with $D = \\text{diag} A$. If $r_i$ is the $i^{th}$ component of the residual, the correction step of the $i^{th}$ variable is $w_i = A_{ii}^{-1} r_i$. All these correction steps are computed for the same residual.\n",
    "\n",
    "In contrast, the Gauss-Seidel iteration updates the $i^{th}$ variable from the residual computed from all up to date variables:\n",
    "\n",
    "for $i = 1, \\ldots, n$: <br>\n",
    "$\\qquad \\hat x_i = x_i + A_{ii}^{-1} \\big( b_i - \\sum_{j=1}^{i-1} A_{ij} \\hat x_j - \\sum_{j=i}^n A_{ij} x_j \\big)$\n",
    "\n",
    "The method depends on the enumeration of variables. E.g, one could loop backwards, and thus obtains the so-called backward Gauss-Seidel iteration.\n",
    "\n",
    "There holds for every $1 \\leq i \\leq n$:\n",
    "$$\n",
    "\\sum_{j=1}^i A_{ij} \\hat x_j = b_i - \\sum_{j=i+1}^n A_{ij} x_j\n",
    "$$\n",
    "\n",
    "If we split the matrix \n",
    "\n",
    "$$\n",
    "A = L + D + R\n",
    "$$\n",
    "\n",
    "with a strictly lower triangular matrix $L$, a diagonal matrix $D$, and a strictly upper diagonal matrix $R$ we can write\n",
    "\n",
    "$$\n",
    "(L+D) \\hat x = b - R x\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "(L+D) \\hat x = (L+D) x + b - A x\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\hat x = x + (L+D)^{-1} (b - A x)\n",
    "$$\n",
    "\n",
    "This means we can interpret the Gauss-Seidel iteration as a preconditioned Richardson iteration, with preconditioner $C = L+D$. In general, this is not a symmetric preconditioner.\n",
    "\n",
    "If we run the backward Gauss-Seidel iteration, the preconditioner becomes $C = D + R$. Now, we combine one step forward, with one step backward Gauss-Seidel:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\tilde x & = & x + (L+D)^{-1} (b - A x) \\\\\n",
    "\\hat x & = & \\tilde x + (D+R)^{-1} (b - A \\tilde x)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    "we can write\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\hat x & = & (D+R)^{-1} (b - L \\tilde x) \\\\\n",
    "& = & (D+R)^{-1} \\big(b - L (L+D)^{-1} (b - R x) \\big) \\\\\n",
    "& = & (D+R)^{-1} \\big( (L+D)(L+D)^{-1} b - L (L+D)^{-1} b \\big) + \\ldots x \\\\\n",
    "& = & (D+R)^{-1} D (L+D)^{-1} b + \\ldots x \\\\\n",
    "& = & x + (D+R)^{-1} D (L+D)^{-1} (b - A x)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "and observe that the combined forward-backward Gauss-Seidel iteration is a Richardson method with preconditioner \n",
    "\n",
    "$$\n",
    "C_{FBGS}^{-1} = (D+R)^{-1} D (L+D)^{-1}.\n",
    "$$\n",
    "\n",
    "If $A$ is symmetric, then $(L+D)^T = D+R$, and $C_{FBGS}$ is symmetric as well. If $A$ is SPD, then we obtain \n",
    "\n",
    "$$\n",
    "x^T C_{FBGS} x \\geq x^T A x  \\qquad \\forall \\, x\n",
    "$$\n",
    "\n",
    "i.e. the spectral constant $\\gamma_2 = 1$, and choosing a damping parameter $\\alpha = 1$ is guaranteed to converge.\n",
    "\n",
    "We prove this by calculation:\n",
    "\\begin{eqnarray*}\n",
    "C_{FBGS} & = & (L+D) D^{-1} (D+R) \\\\\n",
    "& = & L + D + R + L D^{-1} R \\\\\n",
    "& = & A + L D^{-1} R\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Since $x^T L D^{-1} R x = (R x)^T D^{-1} (R x) = \\| R x \\|_{D^{-1}}^2 > 0$ we have proven that\n",
    "\n",
    "$$\n",
    "C_{FBGS} \\geq A\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-windsor",
   "metadata": {},
   "source": [
    "The error propagation matrix of the FB - GS is\n",
    "\n",
    "$$\n",
    "M_{FBGS} = \\underbrace{(I - (D+R)^{-1} A)}_{M_{BGS}} \\underbrace{ ( I - (D+L)^{-1} A) }_{M_{FBS}},\n",
    "$$\n",
    "\n",
    "and has norm $\\| M_{FBGS} \\|_A < 1$. The error propagation matrices\n",
    "of the forward and backward GS are $A$-adjoint to each other:\n",
    "\n",
    "$$\n",
    "\\left< M_{FGS} x, y \\right>_A = \\left< x, M_{BGS} y \\right>_A\n",
    "$$\n",
    "\n",
    "Thus, a single forward (or single backward Gauss-Seidel) step is also convergenct:\n",
    "\n",
    "$$\n",
    "\\left< M_{FGS} x, M_{FGS} x \\right>_A = \\left< M_{BGS} M_{FGS} x, x \\right>_A \\leq \\| M_{FBGS} \\|_A \\| x \\|_A^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-lotus",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
