{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "settled-remedy",
   "metadata": {},
   "source": [
    "The Gradient Method\n",
    "===\n",
    "In this section we assume the $A$ is SPD.\n",
    "\n",
    "We define the function \n",
    "$$\n",
    "f(x) = \\tfrac{1}{2} x^T A x - b^T x.\n",
    "$$\n",
    "\n",
    "The gradient and Hessian matrix of $f$ are\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\nabla f(x) & = & A x - b \\\\\n",
    "\\nabla^2 f & = & A\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Since $A$ is positive definite, $f$ is convex and there exists a unique minimizer of\n",
    "\n",
    "$$\n",
    "\\min_{x \\in {\\mathbb R}} f(x)\n",
    "$$\n",
    "\n",
    "characterized by $\\nabla f = 0$, i.e. the solution of the linear system $A x = b$.\n",
    "\n",
    "\n",
    "There holds\n",
    "\n",
    "$$\n",
    "f(x) = f(x^\\ast) + \\tfrac{1}{2} \\| x - x^\\ast \\|_A^2\n",
    "$$\n",
    "\n",
    "which is simply verified by calculation:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "& & f(x^\\ast) + \\tfrac{1}{2} \\| x - x^\\ast \\|_A^2 \\\\ \n",
    "& = & \\tfrac{1}{2} {x^\\ast}^T A x^\\ast - b^T x^\\ast + \\tfrac{1}{2}(x - x^\\ast)^T A (x - x^\\ast) \\\\\n",
    "& = & x^\\ast A x^\\ast - b^T x^\\ast + \\tfrac{1}{2} x^T A x - x^T A x^\\ast \\\\\n",
    "& = & \\tfrac{1}{2} x^T A x - b^T x = f(x)\n",
    "\\end{eqnarray}\n",
    "\n",
    "The error in energy norm is directly related to the distance to the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-detective",
   "metadata": {},
   "source": [
    "We apply the gradient method. The next iterate $x^{k+1}$ is obtained my moving from $x^k$ into the direction of the negative gradient:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "x^{k+1} & = & x^k - \\alpha \\nabla f(x^k) \\\\\n",
    "& = & x^k + \\alpha r \\qquad \\text{with} \\qquad r = b - A x^k\n",
    "\\end{eqnarray*}\n",
    "\n",
    "The parameter $\\alpha$ can be obtained by line-search:\n",
    "\n",
    "$$\n",
    "\\min_{\\alpha \\in {\\mathbb R}} f(x^k + \\alpha r)\n",
    "$$\n",
    "\n",
    "i.e.\n",
    "\n",
    "$$\n",
    "\\min_\\alpha \\tfrac{1}{2} (x^k + \\alpha r)^T A (x^k + \\alpha r) - b^T (x^k + \\alpha r)\n",
    "$$\n",
    "\n",
    "what is a minimization problem of a convex, quadratic function\n",
    "\n",
    "$$\n",
    "\\min_{\\alpha \\in {\\mathbb R}} \\tfrac{1}{2} r^T A r \\, \\alpha^2 - (b-A x^k)^T r \\, \\alpha + \\tfrac{1}{2} {x^k}^T A x^k\n",
    "$$ \n",
    "\n",
    "The optimal value $\\alpha_\\text{opt}$ is given by\n",
    "$$\n",
    "\\alpha_\\text{opt} = \\frac{r^T r}{r^T A r}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-dance",
   "metadata": {},
   "source": [
    "The gradient method looks like:\n",
    "\n",
    "Given $x^0$ <br>\n",
    "for $k = 0, 1, 2, \\ldots$ <br>\n",
    "$\\qquad r = b - A x^k$ <br>\n",
    "$\\qquad \\alpha = r^T r \\, / \\, r^T A r$ <br>\n",
    "$\\qquad x^{k+1} = x^k + \\alpha r$ <br>\n",
    "\n",
    "In this version, one needs to matrix-vector products with $A$. By updating the residual one can avoid the second product:\n",
    "\n",
    "Given $x^0$ <br>\n",
    "$r^0 = b - A x^0$ <br>\n",
    "for $k = 0, 1, 2, \\ldots$ <br>\n",
    "$\\qquad p = A r^k$ <br>\n",
    "$\\qquad \\alpha = {r^k}^T r^k \\, / \\, {r^k}^T p$ <br>\n",
    "$\\qquad x^{k+1} = x^k + \\alpha r^k$ <br>\n",
    "$\\qquad r^{k+1} = r^k - \\alpha p$ <br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngsolve import *\n",
    "from netgen.geom2d import unit_square\n",
    "mesh = Mesh(unit_square.GenerateMesh(maxh=0.1))\n",
    "fes = H1(mesh, order=1)\n",
    "u,v = fes.TnT()\n",
    "a = BilinearForm(grad(u)*grad(v)*dx+10*u*v*dx).Assemble()\n",
    "f = LinearForm(x*y*v*dx).Assemble()\n",
    "gfu = GridFunction(fes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = f.vec.CreateVector()\n",
    "p = f.vec.CreateVector()\n",
    "\n",
    "gfu.vec[:] = 0\n",
    "r.data = f.vec\n",
    "err0 = Norm(r)\n",
    "its = 0\n",
    "while True:\n",
    "    p.data = a.mat * r\n",
    "    err2 = InnerProduct(r,r)\n",
    "    alpha = err2 / InnerProduct(r,p)\n",
    "\n",
    "    print (\"iteration\", its, \"res=\", sqrt(err2))\n",
    "    gfu.vec.data += alpha * r\n",
    "    r.data -= alpha * p\n",
    "    if sqrt(err2) < 1e-8 * err0 or its > 10000: break\n",
    "    its = its+1\n",
    "print (\"needed\", its, \"iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-gateway",
   "metadata": {},
   "source": [
    "We observe that the gradient method converges similar fast as the Richardson iteration, but without the need of a good chosen relaxation parameter $\\alpha$.\n",
    "\n",
    "The comparison to Richardson iteration allows also to estimate the error reduction of the gradient method. Let \n",
    "$$\n",
    "\\tilde x^{k+1} = x^k - \\alpha_\\text{Rich} (b - A x^k)\n",
    "$$\n",
    "be one step of Richardson. Then\n",
    "\n",
    "$$\n",
    "\\| x^{k+1} - x^\\ast \\|_A^2 = 2 \\, (f(x^{k+1}) - f(x^\\ast)) \n",
    "\\leq 2 \\, ( f (\\tilde x^{k+1}) - f(x^\\ast) ) = \\| \\tilde x^{k+1} - x^\\ast \\|_A^2\n",
    "$$\n",
    "\n",
    "One step of the gradient method reduces the error (measured in energy norm) at least as much as one step of the Richardson method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-perry",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
