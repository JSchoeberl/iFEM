{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55237494",
   "metadata": {},
   "source": [
    "Distributed Meshes and Spaces\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6045306d",
   "metadata": {},
   "source": [
    "Setting up the client and the world-communiactor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a9afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyparallel import Client\n",
    "c = Client()\n",
    "c.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf5e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bcffdb",
   "metadata": {},
   "source": [
    "The master generates a mesh, which is then distributed within the team of processors. The master process calls the graph partitioning library metis, which assigns a process id to each element. Then, elements are sent to the process with according rank.\n",
    "\n",
    "The distribution is done for the Netgen mesh. Parallel uniform refinement of the Netgen-mesh is also possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214f64e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "from ngsolve import *\n",
    "from netgen.geom2d import unit_square\n",
    "\n",
    "if comm.rank == 0:\n",
    "    ngmesh = unit_square.GenerateMesh(maxh=0.1)\n",
    "    print (\"global num els =\", len(ngmesh.Elements2D()))\n",
    "    ngmesh.Distribute(comm)\n",
    "else:\n",
    "    ngmesh = netgen.meshing.Mesh.Receive(comm)\n",
    "\n",
    "for l in range(0):\n",
    "    ngmesh.Refine()\n",
    "    \n",
    "mesh = Mesh(ngmesh)\n",
    "print (\"process\",comm.rank,\"got elements:\",mesh.GetNE(VOL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba0fbdc",
   "metadata": {},
   "source": [
    "Use a the collective communication `reduce` to combine data from each process to one global value. Default reduction operation is summation. Only the root process gets the result. Alternatively, use 'allreduce' to broadcast the result to all team members:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18973795",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "sumup = comm.reduce(mesh.GetNE(VOL))\n",
    "print (\"summing up num els: \", sumup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d4d6ba",
   "metadata": {},
   "source": [
    "We can retriev the `mesh` variable from each node of the cluster. The master process returns the global mesh, each worker only its local part. The list 'meshes' obtains the list of meshes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ff05eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ngsolve import *\n",
    "from ngsolve.webgui import Draw\n",
    "meshes = c[:]['mesh']\n",
    "for m in meshes:\n",
    "    Draw (m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7da656",
   "metadata": {},
   "source": [
    "Distributed Finite element spaces\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9638004",
   "metadata": {},
   "source": [
    "We can define finite element spaces on the distributed mesh. \n",
    "Every process only defines dofs on each subset of elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faedc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "fes = H1(mesh, order=2)\n",
    "print (\"ndof local =\", fes.ndof, \", ndof global =\", fes.ndofglobal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89f54e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "sumlocdofs = comm.reduce (fes.ndof)\n",
    "if comm.rank == 0:\n",
    "    print (\"sum of local dofs:\", sumlocdofs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e820787",
   "metadata": {},
   "source": [
    "The sum of local dofs is larger than the global number of dofs, since dofs at interface nodes are counted multiplel times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ca11bb",
   "metadata": {},
   "source": [
    "We can define distributed grid-functions. Global operations like the `Integrate` function performs local integration, and sum up the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bddba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "gfu = GridFunction(fes)\n",
    "gfu.Set(x*y)\n",
    "print (\"integrate:\", Integrate(gfu, mesh))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668b8be6",
   "metadata": {},
   "source": [
    "We can retrieve the gridfunction to the local Python scope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a69ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "gfus = c[:]['gfu']\n",
    "print (\"integrate locally:\", Integrate(gfus[0], gfus[0].space.mesh))\n",
    "\n",
    "Draw (gfus[0], min=0,max=1)\n",
    "Draw (gfus[1], min=0,max=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fda573e",
   "metadata": {},
   "source": [
    "We can use a piece-wise constant space to visualize the mesh distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd96268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "gfl2 = GridFunction(L2(mesh, order=0))\n",
    "gfl2.vec[:] = comm.rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dc0788",
   "metadata": {},
   "outputs": [],
   "source": [
    "Draw (c[:]['gfl2'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c69fdb",
   "metadata": {},
   "source": [
    "The `ParallelDofs` class\n",
    "---\n",
    "\n",
    "The backbone of connection of dofs is the `ParallelDofs` class. It is provided by a distributed finite element space, based on the connectivity of the mesh. The pardofs object knows with which other processes the dof is shared. We can ask for all dof numbers shared with a particular process, and obtain a list of local dof nrs. The ordering is consistent for both partners. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72f9400",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "pardofs = fes.ParallelDofs()\n",
    "for otherp in range(comm.size):\n",
    "    print (\"with process\", otherp, \"I share dofs\", \\\n",
    "           list(pardofs.Proc2Dof(otherp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692e2255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
