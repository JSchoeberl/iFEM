{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55237494",
   "metadata": {},
   "source": [
    "Distributed Meshes and Spaces\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6045306d",
   "metadata": {},
   "source": [
    "Setting up the client and the world-communiactor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29396fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyparallel import Cluster\n",
    "c = await Cluster(engines=\"mpi\").start_and_connect(n=4, activate=True)\n",
    "c.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf5e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bcffdb",
   "metadata": {},
   "source": [
    "The master generates a mesh, which is then distributed within the team of processors. The master process calls the graph partitioning library metis, which assigns a process id to each element. Then, elements are sent to the process with according rank.\n",
    "\n",
    "The master process does not keep elements itself, it is kept free for special administrative work.\n",
    "\n",
    "The distribution is done for the Netgen mesh. Parallel uniform refinement of the Netgen-mesh is also possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214f64e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "from ngsolve import *\n",
    "\n",
    "if comm.rank == 0:\n",
    "    ngmesh = unit_square.GenerateMesh(maxh=0.1)\n",
    "    print (\"global num els =\", len(ngmesh.Elements2D()))\n",
    "    ngmesh.Distribute(comm)\n",
    "else:\n",
    "    ngmesh = netgen.meshing.Mesh.Receive(comm)\n",
    "\n",
    "for l in range(2):\n",
    "    ngmesh.Refine()\n",
    "    \n",
    "mesh = Mesh(ngmesh)\n",
    "print (\"process\", comm.rank, \"got elements:\", mesh.GetNE(VOL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba0fbdc",
   "metadata": {},
   "source": [
    "The collective communication `reduce` combines data from each process to one global value. Default reduction operation is summation. Only the root process gets the result. Alternatively, use 'allreduce' to broadcast the result to all team members:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18973795",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "sumup = comm.reduce(mesh.GetNE(VOL))\n",
    "print (\"summing up num els: \", sumup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d4d6ba",
   "metadata": {},
   "source": [
    "We can retriev the `mesh` variable from each node of the cluster. The master process returns the global mesh, each worker only its local part. The list 'meshes' obtains the list of meshes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ff05eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngsolve import *\n",
    "from ngsolve.webgui import Draw\n",
    "meshes = c[:]['mesh']\n",
    "for i,m in enumerate(meshes):\n",
    "    print (\"mesh of rank\", i)\n",
    "    Draw (m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7da656",
   "metadata": {},
   "source": [
    "Distributed finite element spaces\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9638004",
   "metadata": {},
   "source": [
    "We can define finite element spaces on the distributed mesh. \n",
    "Every process only defines dofs on its subset of elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faedc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "fes = H1(mesh, order=2)\n",
    "print (\"ndof local =\", fes.ndof, \", ndof global =\", fes.ndofglobal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89f54e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "sumlocdofs = comm.reduce (fes.ndof)\n",
    "if comm.rank == 0:\n",
    "    print (\"sum of local dofs:\", sumlocdofs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e820787",
   "metadata": {},
   "source": [
    "The sum of local dofs is larger than the global number of dofs, since dofs at interface nodes are counted multiplel times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ca11bb",
   "metadata": {},
   "source": [
    "We can define distributed grid-functions. Global operations like the `Integrate` function performs local integration, and sum up the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bddba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "gfu = GridFunction(fes)\n",
    "gfu.Set(x*y)\n",
    "print (\"integrate:\", Integrate(gfu, mesh))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668b8be6",
   "metadata": {},
   "source": [
    "We can retrieve the gridfunction to the local Python scope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a69ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "gfus = c[:]['gfu']\n",
    "print (\"integrate locally:\", Integrate(gfus[0], gfus[0].space.mesh))\n",
    "\n",
    "Draw (gfus[0], min=0,max=1)\n",
    "Draw (gfus[1], min=0,max=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fda573e",
   "metadata": {},
   "source": [
    "We can use a piece-wise constant space to visualize the mesh partitioning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd96268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "gfl2 = GridFunction(L2(mesh, order=0))\n",
    "gfl2.vec[:] = comm.rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dc0788",
   "metadata": {},
   "outputs": [],
   "source": [
    "Draw (c[:]['gfl2'][0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c69fdb",
   "metadata": {},
   "source": [
    "The `ParallelDofs` class\n",
    "---\n",
    "\n",
    "The back-bone of connection of dofs is the `ParallelDofs` class. It is provided by a distributed finite element space, based on the connectivity of the mesh. The pardofs object knows with which other processes the dof is shared. We can ask for all dof numbers shared with a particular process, and obtain a list of local dof nrs. The ordering is consistent for both partners. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72f9400",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "pardofs = fes.ParallelDofs()\n",
    "for otherp in range(comm.size):\n",
    "    print (\"with process\", otherp, \"I share dofs\", \\\n",
    "           list(pardofs.Proc2Dof(otherp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692e2255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8798531b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5873f96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
