{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "settled-remedy",
   "metadata": {},
   "source": [
    "The Gradient Method\n",
    "===\n",
    "In this section we assume that $A$ is SPD.\n",
    "\n",
    "We define the function $f : {\\mathbb R}^n \\rightarrow {\\mathbb R}$ as\n",
    "\n",
    "$$\n",
    "f(x) = \\tfrac{1}{2} x^T A x - b^T x.\n",
    "$$\n",
    "\n",
    "The gradient and Hessian matrix of $f$ are\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla f(x) & = A x - b, \\\\\n",
    "\\nabla^2 f & = A.\n",
    "\\end{align*}\n",
    "\n",
    "Since $A$ is positive definite, $f$ is convex and there exists a unique minimizer of\n",
    "\n",
    "$$\n",
    "\\min_{x \\in {\\mathbb R}^n} f(x)\n",
    "$$\n",
    "\n",
    "characterized by $\\nabla f = 0$, i.e. the solution of the linear system $A x = b$. Thus, the linear system is equivalent to the minimization problem.\n",
    "\n",
    "\n",
    "There holds\n",
    "\n",
    "$$\n",
    "f(x) = f(x^\\ast) + \\tfrac{1}{2} \\| x - x^\\ast \\|_A^2,\n",
    "$$\n",
    "\n",
    "which is simply verified by calculation:\n",
    "\n",
    "\\begin{align*}\n",
    "f(x^\\ast) + \\tfrac{1}{2} \\| x - x^\\ast \\|_A^2  \n",
    "& =  \\tfrac{1}{2} {x^\\ast}^T A x^\\ast - b^T x^\\ast + \\tfrac{1}{2}(x - x^\\ast)^T A (x - x^\\ast) \\\\\n",
    "& =  {x^\\ast}^T A x^\\ast - b^T x^\\ast + \\tfrac{1}{2} x^T A x - x^T A x^\\ast \\\\\n",
    "& =  \\tfrac{1}{2} x^T A x - b^T x = f(x)\n",
    "\\end{align*}\n",
    "\n",
    "The error in energy norm is directly related to the distance to the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-detective",
   "metadata": {},
   "source": [
    "We apply the gradient method: The next iterate $x^{k+1}$ is obtained by moving from $x^k$ into the direction of the negative gradient:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "x^{k+1} & = & x^k - \\alpha \\nabla f(x^k) \\\\\n",
    "& = & x^k + \\alpha r \\qquad \\text{with} \\qquad r = b - A x^k\n",
    "\\end{eqnarray*}\n",
    "\n",
    "The optimal parameter $\\alpha$ can be obtained by line-search:\n",
    "\n",
    "$$\n",
    "\\min_{\\alpha \\in {\\mathbb R}} f(x^k + \\alpha r)\n",
    "$$\n",
    "\n",
    "i.e.\n",
    "\n",
    "$$\n",
    "\\min_\\alpha \\tfrac{1}{2} (x^k + \\alpha r)^T A (x^k + \\alpha r) - b^T (x^k + \\alpha r)\n",
    "$$\n",
    "\n",
    "what is a minimization problem of a convex, quadratic function\n",
    "\n",
    "$$\n",
    "\\min_{\\alpha \\in {\\mathbb R}} \\tfrac{1}{2} r^T A r \\, \\alpha^2 - (b-A x^k)^T r \\, \\alpha + \\tfrac{1}{2} {x^k}^T A x^k - b^T x^k\n",
    "$$ \n",
    "\n",
    "The optimal value $\\alpha_\\text{opt}$ is found as\n",
    "\n",
    "$$\n",
    "\\alpha_\\text{opt} = \\frac{r^T r}{r^T A r}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-dance",
   "metadata": {},
   "source": [
    "The gradient method looks like:\n",
    "\n",
    "> Given $x^0$ <br>\n",
    "> for $k = 0, 1, 2, \\ldots$ <br>\n",
    "> $\\qquad r = b - A x^k$ <br>\n",
    "> $\\qquad \\alpha = \\frac{r^T r}{r^T A r}$ <br>\n",
    "> $\\qquad x^{k+1} = x^k + \\alpha r$ <br>\n",
    "\n",
    "In this version, one needs two matrix-vector products with $A$. By updating the residual one can avoid the second product:\n",
    "\n",
    "> Given $x^0$ <br>\n",
    "> $r^0 = b - A x^0$ <br>\n",
    "> for $k = 0, 1, 2, \\ldots$ <br>\n",
    "> $\\qquad p = A r^k$ <br>\n",
    "> $\\qquad \\alpha = \\frac{{r^k}^T r^k}{{r^k}^T p}$ <br>\n",
    "> $\\qquad x^{k+1} = x^k + \\alpha r^k$ <br>\n",
    "> $\\qquad r^{k+1} = r^k - \\alpha p$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngsolve import *\n",
    "mesh = Mesh(unit_square.GenerateMesh(maxh=0.1))\n",
    "fes = H1(mesh, order=1)\n",
    "u,v = fes.TnT()\n",
    "a = BilinearForm(grad(u)*grad(v)*dx+10*u*v*dx).Assemble()\n",
    "f = LinearForm(x*y*v*dx).Assemble()\n",
    "gfu = GridFunction(fes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26afb0bf-80d7-4301-aaae-03e4bd38f183",
   "metadata": {},
   "source": [
    "The code in NGSolve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = f.vec.CreateVector()\n",
    "p = f.vec.CreateVector()\n",
    "\n",
    "gfu.vec[:] = 0\n",
    "r.data = f.vec\n",
    "err0 = Norm(r)\n",
    "its = 0\n",
    "errhist = []\n",
    "while True:\n",
    "    p.data = a.mat * r\n",
    "    err2 = InnerProduct(r,r)\n",
    "    alpha = err2 / InnerProduct(r,p)\n",
    "\n",
    "    # print (\"iteration\", its, \"res=\", sqrt(err2))\n",
    "    errhist.append (sqrt(err2))\n",
    "    gfu.vec.data += alpha * r\n",
    "    r.data -= alpha * p\n",
    "    if sqrt(err2) < 1e-8 * err0 or its > 10000: break\n",
    "    its = its+1\n",
    "print (\"needed\", len(errhist), \"iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4413183a-6bcb-4f90-9d23-4af060a77e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.yscale('log')\n",
    "plt.plot(errhist);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-gateway",
   "metadata": {},
   "source": [
    "We observe that the gradient method converges similar fast as the Richardson iteration, but without the need of a good chosen relaxation parameter $\\alpha$.\n",
    "\n",
    "The comparison to Richardson iteration allows also to estimate the error reduction of the gradient method. Let \n",
    "$$\n",
    "\\tilde x^{k+1} = x^k - \\alpha_\\text{Rich} (b - A x^k)\n",
    "$$\n",
    "be one step of Richardson. Then, using the optimality property along the search direction of the gradient method, we observe\n",
    "\n",
    "\\begin{align*}\n",
    "\\| x^{k+1} - x^\\ast \\|_A^2 &= 2 \\, (f(x^{k+1}) - f(x^\\ast)) \n",
    "\\leq 2 \\, ( f (\\tilde x^{k+1}) - f(x^\\ast) ) \\\\\n",
    " &= \\| \\tilde x^{k+1} - x^\\ast \\|_A^2 \\leq \\rho_{Rich} \\| x^k - x^\\ast \\|_A\n",
    "\\end{align*}\n",
    "\n",
    "One step of the gradient method reduces the error (measured in energy norm) at least as much as one step of the Richardson method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-perry",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
