{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "demographic-baker",
   "metadata": {},
   "source": [
    "# The Richardson Iteration\n",
    "\n",
    "also called simple iteration is the fixed point iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-dining",
   "metadata": {},
   "source": [
    "$$\n",
    "x^{k+1} := x^k + \\alpha \\, (b - A x^k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-commander",
   "metadata": {},
   "source": [
    "with an arbitrary starting value $x^0 \\in {\\mathbb R}^n$, and a properly cosen damping parameter $\\alpha$. The misfit of the equation, $r = b - A x^k$ is called residual.\n",
    "\n",
    "The solution $x^\\ast$ is a fixed point of the iteration (since $b - A x^\\ast = 0$).\n",
    "\n",
    "If we define the error as\n",
    "\n",
    "$$\n",
    "e^k = x^k - x^\\ast,\n",
    "$$\n",
    "\n",
    "the error propagation from one step to the next is\n",
    "\n",
    "\\begin{align*}\n",
    "e^{k+1} & = x^{k+1} - x^\\ast = x^k + \\alpha (b - A x^k) - x^\\ast \\\\\n",
    "& = x^k - x^\\ast + \\alpha A (x^\\ast - x^k) = (I - \\alpha A) (x^k - x^\\ast)\n",
    "\\end{align*}\n",
    "\n",
    "This means the new error is obtained from the old error by the error propagation matrix\n",
    "\n",
    "$$\n",
    "e^{k+1} = \\underbrace{(I - \\alpha A)}_{M} \\, e^k\n",
    "$$\n",
    "\n",
    "Two strategies to verify convergence are:\n",
    "\n",
    "* prove that the spectral radius \n",
    "\n",
    "$$\n",
    "\\rho(I - \\alpha A) := \\max_{\\lambda \\in \\sigma(I - \\alpha A)} |\\lambda| < 1\n",
    "$$\n",
    "\n",
    "* find some norm $\\| \\cdot \\|$ such that the matrix norm (=operator norm)\n",
    "\n",
    "$$\n",
    "\\| I - \\alpha A \\| := \\sup_{x \\in {\\mathbb R}^n} \\frac{ \\| (I - \\alpha A) x \\| }{ \\| x \\| } < 1\n",
    "$$\n",
    "\n",
    "The first one, $\\rho < 1$, only provides asymptotic convergence. This is easily proven if A is diagonizable, i.e. it features a full set of eigenvectors $z^j$ and eigenvalues $\\lambda_j$. Expand the initial error as\n",
    "\n",
    "$$\n",
    "e^0 = \\sum_j e^0_j z^j,\n",
    "$$\n",
    "then\n",
    "\n",
    "$$\n",
    "e^k = \\sum_j (1-\\alpha \\lambda_j)^k e^0_j z^j\n",
    "$$\n",
    "and\n",
    "\n",
    "$$\n",
    "\\| e^k \\| \\leq \\rho^k  \\sum_j | e^0_j | \\| z^j \\|\n",
    "$$\n",
    "This means $\\| e^k \\| \\leq C \\rho^k$, but the error does not have to decrease monotonically.\n",
    "\n",
    "However, if $\\| I - \\alpha A \\| < 1$, then\n",
    "\n",
    "$$\n",
    "\\| e^{k+1} \\| \\leq \\| I - \\alpha A \\| \\, \\| e^k \\|,\n",
    "$$\n",
    "proves that the error decreases in every iteration step. Note that the matrix norm is the operator norm generated by the vector norm.\n",
    "\n",
    "Some facts:\n",
    "* If the norm $\\| \\cdot \\|$ is generated by an inner product $\\left< \\cdot, \\cdot \\right>$ (parallelogram identity), and $M$ is some self adjoint matrix with respect to this inner product, i.e.\n",
    "  \n",
    "  $$\n",
    "  \\left< M x, y \\right>  = \\left< x, M y \\right>, \n",
    "  $$\n",
    "  then $\\rho(M) = \\| M \\|$\n",
    "\n",
    "*  If $\\left< \\cdot , \\cdot \\right>$ is the Euklidean inner product, then $M$ is self-adjoint exactly when $M$ is symmetric.\n",
    "  \n",
    "* The spectral radius is bounded by every operator norm. There exists some norm such that the operator norm is arbitrary close to the spectral radius, i.e.\n",
    "  \n",
    "  $$\n",
    "  \\rho(M) = \\inf_{ \\text{norms} \\| \\cdot \\| } \\| M \\|\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-recording",
   "metadata": {},
   "source": [
    "## Optimizing the relaxation parameter $\\alpha$\n",
    "\n",
    "Let $A$ be SPD, and let $\\sigma(A) = \\{ \\lambda_i \\in {\\mathbb R} \\}$ with $0 < \\lambda_1 \\leq \\lambda_2 \\ldots \\leq \\lambda_n$.\n",
    "\n",
    "Then the eigenvalues of the error propagation matrix $M = I - \\alpha A$ are $\\{ 1 - \\alpha \\lambda_i  \\}$. \n",
    "\n",
    "Whenever we choose \n",
    "\n",
    "$$\n",
    "0 < \\alpha < \\frac{2}{\\lambda_n}\n",
    "$$\n",
    "we obtain $\\rho(M) < 1$, and a convergent iteration.\n",
    "\n",
    "\n",
    "The spectral radius of $M$ is \n",
    "\n",
    "$$\n",
    "\\rho(M) = \\max_i \\{ | 1 - \\alpha \\lambda_i| \\}  = \n",
    "\\max \\{ 1 - \\alpha \\lambda_1, - (1-\\alpha \\lambda_n) \\}\n",
    "$$\n",
    "\n",
    "The maximum is minimized if we choose $\\alpha$ optimally such that\n",
    "\n",
    "$$\n",
    "1 - \\alpha \\lambda_1 = - (1 - \\alpha \\lambda_n),\n",
    "$$\n",
    "i.e.\n",
    "\n",
    "$$\n",
    "\\alpha_{\\text{opt}} = \\frac{2}{\\lambda_1 + \\lambda_n} \n",
    "$$\n",
    "leading to the optimal rate of convergence\n",
    "\n",
    "$$\n",
    "\\rho_{\\text{opt}} = \\frac{\\lambda_n - \\lambda_1}{\\lambda_n+\\lambda_1}\n",
    "\\approx 1 - 2 \\frac{\\lambda_1}{\\lambda_n} = 1 - \\frac{2}{\\kappa(A)}\n",
    "$$\n",
    "\n",
    "with the condition number $\\kappa(A) = \\lambda_n(A) / \\lambda_1(A)$.\n",
    "\n",
    "\n",
    "After $N$ iterations, the error is reduced by \n",
    "\n",
    "$$\n",
    "\\left(1 - \\frac{1}{\\kappa}\\right)^N.\n",
    "$$\n",
    "\n",
    "To obtain an error reduction by by a factor of $\\varepsilon$ one needs\n",
    "\n",
    "$$\n",
    "N = \\frac{\\log \\varepsilon}{\\log (1-1/\\kappa)} \\approx \\kappa \\, | \\log \\varepsilon |\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-mount",
   "metadata": {},
   "source": [
    "## Experiments with the Richardson iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-lancaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngsolve import *\n",
    "mesh = Mesh(unit_square.GenerateMesh(maxh=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "fes = H1(mesh, order=1)\n",
    "u,v = fes.TnT()\n",
    "a = BilinearForm(grad(u)*grad(v)*dx+10*u*v*dx).Assemble()\n",
    "f = LinearForm(x*y*v*dx).Assemble()\n",
    "gfu = GridFunction(fes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-paraguay",
   "metadata": {},
   "source": [
    "we determine (an approximation to) the largest eigenvalue by a few steps of the power iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv = gfu.vec.CreateVector()\n",
    "hv2 = gfu.vec.CreateVector()\n",
    "hv.SetRandom()\n",
    "hv.data /= Norm(hv)\n",
    "for k in range(20):\n",
    "    hv2.data = a.mat * hv\n",
    "    rho = Norm(hv2)\n",
    "    print (rho)\n",
    "    hv.data = 1/rho * hv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db2b71b-c72e-4303-970b-2bccca67eefc",
   "metadata": {},
   "source": [
    "Run the Richardson iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-wells",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1 / rho\n",
    "r = f.vec.CreateVector()\n",
    "gfu.vec[:] = 0\n",
    "err0 = Norm(f.vec)\n",
    "its = 0\n",
    "errhist = []\n",
    "while True:\n",
    "    r.data = f.vec - a.mat * gfu.vec\n",
    "    err = Norm(r)\n",
    "    print (\"iteration\", its, \"res=\", err)\n",
    "    errhist.append (err)\n",
    "    gfu.vec.data += alpha * r\n",
    "    if err < 1e-8 * err0 or its > 10000: break\n",
    "    its = its+1\n",
    "print (\"needed\", its, \"iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c399c5c7-f1dd-493d-82bf-7b450cb14063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.yscale('log')\n",
    "plt.plot (errhist);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-orientation",
   "metadata": {},
   "source": [
    "Experimenting with the mesh-size $h$, we observe experimentally that the number of iterations and thus the condition number grow proportional to $h^{-2}$. This can be easily calculated for a finite difference discretization on a uniform mesh. We will prove it for finite elements later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-struggle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngsolve.webgui import Draw\n",
    "Draw (gfu);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8cfe0d-aa47-4237-b708-9b6d823f8bcd",
   "metadata": {},
   "source": [
    "**Excercise:** \n",
    "* experiment with the relaxation parameter $\\alpha$, how do the iteration numbers change ?\n",
    "* halve the mesh-size, how do the iteration number change ?\n",
    "* modify the parameter in front of the `u*v` term of the bilinear form. How does the iteration number change ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e543ee1b-252b-43ab-ad73-3f729e6c7aef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
